{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "j5o3srn4kqqgj3fes5jo",
   "authorId": "322325853055",
   "authorName": "CROMANO",
   "authorEmail": "chase.romano@snowflake.com",
   "sessionId": "c2c1915d-82e1-4b58-af69-6997d4967937",
   "lastEditTime": 1750876359009
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "62f9bb7f-d1c8-43d2-841b-9b69574c1fc8",
   "metadata": {
    "name": "search_md",
    "collapsed": false
   },
   "source": "### Create the Search Service"
  },
  {
   "cell_type": "code",
   "id": "ca8526e7-89fd-41c2-a2b0-7f85b52d5a2c",
   "metadata": {
    "language": "sql",
    "name": "Create_stage"
   },
   "outputs": [],
   "source": "-- Create a stage for docs\n\nCREATE STAGE if not exists agent_materials \n\tDIRECTORY = ( ENABLE = true ) \n\tENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE' );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d7d3763-7db4-4279-ae5e-1f95a3678412",
   "metadata": {
    "language": "python",
    "name": "Upload_pdf"
   },
   "outputs": [],
   "source": "MY_STAGE = 'agent_materials'\nMY_FILE_NAME = \"snowflake_10k_2025.pdf\"\n\n# Upload the file to a stage.\nput_result = session.file.put(MY_FILE_NAME, MY_STAGE, auto_compress=False,overwrite=True)\nput_result[0].status",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f45a6bda-52e1-45e0-83ad-e464b002e64f",
   "metadata": {
    "language": "sql",
    "name": "Perform_OCR"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE raw_text_10k AS\nSELECT\n    RELATIVE_PATH,\n    TO_VARCHAR (\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n            '@agent_materials',\n            RELATIVE_PATH,\n            {'mode': 'LAYOUT'} ):content\n        ) AS EXTRACTED_LAYOUT\nFROM\n    DIRECTORY('@agent_materials')\nWHERE\n    RELATIVE_PATH LIKE '%.pdf';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d41a7dcd-37cc-45e2-ad3c-47fe7f427889",
   "metadata": {
    "language": "sql",
    "name": "Chunk_Vectorize"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE doc_chunks_10k AS\nSELECT\n    relative_path,\n    BUILD_SCOPED_FILE_URL(@docs, relative_path) AS file_url,\n    CONCAT(relative_path, ': ', c.value::TEXT) AS chunk,\n    'English' AS language\nFROM\n    raw_text_10k,\n    LATERAL FLATTEN(SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n        EXTRACTED_LAYOUT,\n        'markdown',\n        2000, -- chunks of 2000 characters\n        300 -- 300 character overlap\n    )) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e29336e5-262e-44a6-8e0c-22e843951b9b",
   "metadata": {
    "language": "python",
    "name": "Get_Warehouse"
   },
   "outputs": [],
   "source": "# This cell will give you your current warehouse\n# Replace line 4 in the cell below with your warehouse\n\ncurrent_warehouse = session.get_current_warehouse()\nprint(current_warehouse)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83798031-8076-45e5-beac-4c16a4b894b3",
   "metadata": {
    "language": "sql",
    "name": "Create_Search_Service"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE snow_10k_2025\n    ON chunk\n    ATTRIBUTES language\n    WAREHOUSE = wh_xs\n    TARGET_LAG = '1 day'\n    AS (\n    SELECT\n        chunk,\n        relative_path,\n        file_url,\n        language\n    FROM doc_chunks_10k\n    );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d79065f0-57d0-4f9e-95d8-edfba18b39a8",
   "metadata": {
    "name": "table_md",
    "collapsed": false
   },
   "source": "Create Table for cortex analyst"
  },
  {
   "cell_type": "code",
   "id": "96a16d8b-3b41-4fa3-bb75-525955629f49",
   "metadata": {
    "language": "python",
    "name": "Upload_data"
   },
   "outputs": [],
   "source": "MY_FILE_NAME = \"SP500.csv\"\n\n# Upload the file to a stage.\nput_result = session.file.put(MY_FILE_NAME, MY_STAGE, auto_compress=False,overwrite=True)\nput_result[0].status",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8115b333-2557-4aeb-ac7e-48901724441e",
   "metadata": {
    "language": "sql",
    "name": "Create_table"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE SP_500 ( EXCHANGE VARCHAR , SYMBOL VARCHAR , SHORTNAME VARCHAR , LONGNAME VARCHAR , SECTOR VARCHAR , INDUSTRY VARCHAR , CURRENTPRICE NUMBER(38, 3) , MARKETCAP NUMBER(38, 0) , EBITDA NUMBER(38, 0) , REVENUEGROWTH NUMBER(38, 3) , CITY VARCHAR , STATE VARCHAR , COUNTRY VARCHAR , FULLTIMEEMPLOYEES NUMBER(38, 0) , LONGBUSINESSSUMMARY VARCHAR , WEIGHT NUMBER(38, 20) ); \n\nCREATE OR REPLACE TEMP FILE FORMAT SP_LOAD\n\tTYPE=CSV\n    SKIP_HEADER=1\n    FIELD_DELIMITER=','\n    TRIM_SPACE=TRUE\n    FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n    REPLACE_INVALID_CHARACTERS=TRUE\n    DATE_FORMAT=AUTO\n    TIME_FORMAT=AUTO\n    TIMESTAMP_FORMAT=AUTO; \n\nCOPY INTO SP_500\nFROM (SELECT $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16\n\tFROM '@AGENT_MATERIALS') \nFILES = ('SP500.csv') \nFILE_FORMAT = 'SP_LOAD' \nON_ERROR=ABORT_STATEMENT ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "832e967f-1c33-4e2e-ac67-28dce8520972",
   "metadata": {
    "name": "YAML_md",
    "collapsed": false
   },
   "source": "Now that we have the Search Service and the table for Cortex Analyst all we have left to do is create the Analyst Semantic View.  \n\nYou can use the yaml provided or try it out yourself in the UI by navigating to Cortex Analyst under AI/ML and building it yourself"
  },
  {
   "cell_type": "code",
   "id": "fd976cf5-8ebd-438c-94cc-ac91dd4bdfb0",
   "metadata": {
    "language": "python",
    "name": "Upload_yaml"
   },
   "outputs": [],
   "source": "MY_FILE_NAME = \"SP500.yaml\"\n\n# Upload the file to a stage.\nput_result = session.file.put(MY_FILE_NAME, MY_STAGE, auto_compress=False,overwrite=True)\nput_result[0].status",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fdbd1bb-96df-4f0e-98b0-f6485f325035",
   "metadata": {
    "language": "python",
    "name": "upload_agent_gateway"
   },
   "outputs": [],
   "source": "MY_FILE_NAME = \"agent_gateway.zip\"\n\n# Upload the file to a stage.\nput_result = session.file.put(MY_FILE_NAME, MY_STAGE, auto_compress=False,overwrite=True)\nput_result[0].status",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "addee527-0da0-4bda-b8e4-7147634dfb42",
   "metadata": {
    "name": "Analyst_Steps",
    "collapsed": false
   },
   "source": "Open up Streamlit in Snowflake\nCreate a new app on an XS Warehouse\nOnce in the app add the agent_gateway.zip file to Staged Packages in the top right corner\nin the packages in the top left add aiohttp, pydantic\n\n\nWe just uploaded the zip file to the stage.  My stage is in CROMANO.DEMO so when it ask for path replace my Database and Schema with yours.  For example my location is @CROMANO.DEMO.AGENT_MATERIALS/agent_gateway.zip"
  }
 ]
}